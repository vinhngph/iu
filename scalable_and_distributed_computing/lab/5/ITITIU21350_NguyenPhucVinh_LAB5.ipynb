{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H58RnChZq0b"
      },
      "source": [
        "## Exercise I\n",
        "\n",
        "The input is a textual csv file containing the daily value of PM10 for a set of sensors, and in each line of the files has the following format:\n",
        "```sensorId,date,PM10 value (μg/m3)\\n```\n",
        "\n",
        "Here is the example of data:\n",
        "```\n",
        "s1,2016-01-01,20.5\n",
        "s2,2016-01-01,30.1\n",
        "s1,2016-01-02,60.2\n",
        "s2,2016-01-02,20.4\n",
        "s1,2016-01-03,55.5\n",
        "s2,2016-01-03,52.5\n",
        "```\n",
        "\n",
        "You're required to use pyspark to load the file, filter the values and use map/reduce code idea to give the output. The output is a line for each sensor on the standard output.\n",
        "Each line contains a `sensorId` and the list of `dates` with a PM10 values greater than 50 for that sensor. The example output:\n",
        "```\n",
        "(s1, [2016-01-02, 2016-01-03])\n",
        "(s2, [2016-01-03])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/jdk-11.0.26-oracle-x64\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.session import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "labels = [\n",
        "    (\"sensorId\", StringType()),\n",
        "    (\"date\", DateType()),\n",
        "    (\"PM10 value (μg/m3)\", FloatType()),\n",
        "]\n",
        "\n",
        "schema = StructType([StructField(x[0], x[1]) for x in labels])\n",
        "df = spark.read.csv(\"sensor.csv\", sep=\",\", schema=schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('s1', ['2016-01-02', '2016-01-03', '2016-01-04', '2016-01-07']), ('s2', ['2016-01-03', '2016-01-07', '2016-01-09']), ('s3', ['2016-01-05', '2016-01-06', '2016-01-08', '2016-01-09']), ('s4', ['2016-01-05', '2016-01-06', '2016-01-10'])]\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "\n",
        "def map_pm10(data_frame):\n",
        "    df_filter = data_frame.filter(col(\"PM10 value (μg/m3)\") > 50)\n",
        "    return df_filter.rdd.map(lambda row: (row[\"sensorId\"], row[\"date\"].isoformat()))\n",
        "\n",
        "\n",
        "def reduce_pm10(rdd_mapped):\n",
        "    return rdd_mapped.groupByKey().mapValues(list)\n",
        "\n",
        "\n",
        "print(reduce_pm10(map_pm10(df)).collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jgu0vQKVbqDf"
      },
      "source": [
        "## Exercise II\n",
        "\n",
        "Using the same data of the Exercise I, you're required to get the output: sensors ordered by the number of critical days. Each line of the output contains the number of days with a PM10 values greater than 50 for a sensor `s` and the `sensorId` of sensor `s`.\n",
        "\n",
        "The example of the output:\n",
        "```\n",
        "2, s1\n",
        "1, s2\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4, s1\n",
            "4, s3\n",
            "3, s2\n",
            "3, s4\n"
          ]
        }
      ],
      "source": [
        "def map_pm10_ordered(data_frame):\n",
        "    df_filter = data_frame.filter(col(\"PM10 value (μg/m3)\") > 50)\n",
        "    return df_filter.rdd.map(lambda row: (row[\"sensorId\"], row[\"date\"].isoformat()))\n",
        "\n",
        "\n",
        "def reduce_pm10_ex2(rdd_mapped):\n",
        "    groupped = rdd_mapped.groupByKey().mapValues(list).collect()\n",
        "    return [\n",
        "        (len(row[1]), row[0])\n",
        "        for row in sorted(groupped, key=lambda x: len(x[1]), reverse=True)\n",
        "    ]\n",
        "\n",
        "\n",
        "result = reduce_pm10_ex2(map_pm10_ordered(df))\n",
        "for value, id in result:\n",
        "    print(f\"{value}, {id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADGjGNWKePfN"
      },
      "source": [
        "## Exercise III\n",
        "\n",
        "In this exercise, you're given an input: A CSV file containing a list of profiles\n",
        "\n",
        "- Header: `name,age,gender`\n",
        "- Each line of the file contains the information about one user\n",
        "\n",
        "The example of input data\n",
        "```\n",
        "name,surname,age\n",
        "Paolo,Garza,42\n",
        "Luca,Boccia,41\n",
        "Maura,Bianchi,16\n",
        "```\n",
        "\n",
        "You're required to use pyspark to load and analyze the data to achieve the output: A CSV file containing one line for each profile. The original age attribute is substituted with a new attributed called rangeage of type String.\n",
        "```\n",
        "rangeage = \"[\" + (age/10)*10 + \"-\" + (age/10)*10+9 + \"]\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------+--------+\n",
            "| name| surname|rangeage|\n",
            "+-----+--------+--------+\n",
            "|Paolo|   Garza| [40-49]|\n",
            "| Luca|  Boccia| [40-49]|\n",
            "|Maura| Bianchi| [10-19]|\n",
            "|Alice|   Cochi| [10-19]|\n",
            "|Laura|  Latini| [20-29]|\n",
            "|Paula| Zachini| [10-19]|\n",
            "|Carta|  Cianci| [20-29]|\n",
            "| Rita|Lisatini| [30-39]|\n",
            "+-----+--------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import concat, lit, floor\n",
        "\n",
        "labels = [(\"name\", StringType()), (\"surname\", StringType()), (\"age\", IntegerType())]\n",
        "schema = StructType([StructField(x[0], x[1]) for x in labels])\n",
        "df = spark.read.csv(\"person.csv\", header=True, schema=schema)\n",
        "\n",
        "df_output = df.withColumn(\n",
        "    \"rangeage\",\n",
        "    concat(\n",
        "        lit(\"[\"),\n",
        "        floor(col(\"age\") / 10) * 10,\n",
        "        lit(\"-\"),\n",
        "        floor(col(\"age\") / 10) * 10 + 9,\n",
        "        lit(\"]\"),\n",
        "    ),\n",
        ")\n",
        "df_output = df_output.select(\"name\", \"surname\", \"rangeage\")\n",
        "df_output.show()\n",
        "df_output.write.csv(\"output.csv\", header=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
